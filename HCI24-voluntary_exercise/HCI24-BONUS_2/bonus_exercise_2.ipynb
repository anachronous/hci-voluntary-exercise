{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f2e3b99-f78a-46af-baa2-3949afbaf1c1",
   "metadata": {},
   "source": [
    "# HCI Course 2024, ETH Zurich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9e666e-cc88-4c09-80e2-9a423d3dde3c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<h3> DEADLINE: 6th of January 2024 </h3>\n",
    "\n",
    "Make sure this is in your github repo. Make sure that all the answers are saved within, so that we do not have to rerun your code.\n",
    "Your code should be able to run, purely from this notebook.  \n",
    "\n",
    "This is **part 2** of the bonus exercise. **Two bonus exercises combined are 25P in total**. The max bonus on the project grade is +0.25 for the full 25 points. You need to complete both exercise correctly to get the 25P.\n",
    "\n",
    "This notebook wont be part of the exam, but the lecture on Bayesian optimization will be. Therefore this notebook could provide additional practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8783f747-0549-4d3d-bebe-f21e29d26b96",
   "metadata": {},
   "source": [
    "## Bonus Exercise 2: Bayesian Optimization for UI Optimization and Adaptation [10P in total]\n",
    "### Yi-Chi Liao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6579a2-6a41-4cbc-aeb0-31dd5a556048",
   "metadata": {},
   "source": [
    "## 1. Installation, import, and visualization\n",
    "\n",
    "Before running the notebook, make sure you install the libraries correctly. We just need numpy, scipy, and matplotlib. If you don't have them yet, you can install them via the below two ways.\n",
    "\n",
    "1. *pip install -r requirements.txt* \n",
    "2. *conda env create -f environment.yml*\n",
    "\n",
    "If you have any questions regarding installation, please contact Yi-Chi (yichi.liao@inf.ethz.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91908033-c026-4426-b11e-ab2e0b9f74bf",
   "metadata": {},
   "source": [
    "Just import and plot functions. Do not change anything in this seciton.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa381ee-5b90-43da-b198-c8f79a604f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import sin\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(np.__version__)\n",
    "print(scipy.__version__)\n",
    "print(matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e8b444-d954-4ad1-a61d-0a0f607e8457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp_and_acquisition(true_function, gp_model, sample_x_list, sample_y_list, sample_points, acquisition_values=None, acq_x_range=None):\n",
    "    \"\"\"\n",
    "    Plot the true function values, GP mean, GP variance, and acquisition function values in a 1D Bayesian Optimization setting.\n",
    "    \"\"\"\n",
    "    # Calculate true function values over the specified sample points\n",
    "    true_y = true_function(sample_points)\n",
    "    \n",
    "    # GP posterior predictions\n",
    "    gp_y, gp_std = gp_model.predict(sample_points.reshape(-1, 1))\n",
    "    \n",
    "    # Plot observations and GP posterior\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    \n",
    "    # Plot the GP mean and confidence interval\n",
    "    axs[0].fill_between(\n",
    "        sample_points,\n",
    "        (gp_y - 1.96 * gp_std).ravel(),\n",
    "        (gp_y + 1.96 * gp_std).ravel(),\n",
    "        alpha=0.3,\n",
    "        label=\"95% confidence interval\",\n",
    "    )\n",
    "    axs[0].plot(sample_points, true_y, label='True Function', color='black')\n",
    "    axs[0].plot(sample_points, gp_y, label='GP Mean', color='blue')\n",
    "    \n",
    "    # Plot sampled points, highlighting the latest sample in red\n",
    "    for i, (x, y) in enumerate(zip(sample_x_list, sample_y_list)):\n",
    "        color = 'red' if i == len(sample_x_list) - 1 else 'blue'\n",
    "        axs[0].scatter(x, y, color=color)\n",
    "    \n",
    "    axs[0].set_xlabel('x')\n",
    "    axs[0].set_ylabel('Objective Function')\n",
    "    axs[0].set_title('Observation')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "    \n",
    "    # Plot acquisition function if provided\n",
    "    if acquisition_values is not None and acq_x_range is not None:\n",
    "        axs[1].plot(acq_x_range, acquisition_values, color='purple')\n",
    "        axs[1].set_xlabel('x')\n",
    "        axs[1].set_ylabel('Acquisition Function Value')\n",
    "        axs[1].set_title('Acquisition Function')\n",
    "        axs[1].grid(True)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20918dd5-45e2-489e-b350-30a419428517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_gp_and_acquisition(true_function, gp_model, sample_x_list, sample_y_list, bounds, acquisition_function, **acq_kwargs):\n",
    "    \"\"\"\n",
    "    Plot 2D heatmaps for the true function values, GP mean, GP variance, and acquisition function values in a 2D Bayesian Optimization setting.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define a grid within the bounds for plotting\n",
    "    x = np.linspace(bounds[0][0], bounds[0][1], 50)\n",
    "    y = np.linspace(bounds[1][0], bounds[1][1], 50)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    xy_grid = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "\n",
    "    # Calculate true function values over the grid\n",
    "    Z_true = np.array([true_function(point) for point in xy_grid]).reshape(X.shape)\n",
    "\n",
    "    # Calculate GP mean and variance over the grid\n",
    "    Z_gp_mean, Z_gp_std = gp_model.predict(xy_grid)\n",
    "    Z_gp_mean = Z_gp_mean.reshape(X.shape)\n",
    "    Z_gp_var = (Z_gp_std ** 2).reshape(X.shape)\n",
    "\n",
    "    # Calculate acquisition values over the grid\n",
    "    Z_acq = np.array([float(acquisition_function(point.reshape(1, -1), gp_model, np.max(sample_y_list), **acq_kwargs)) for point in xy_grid]).reshape(X.shape)\n",
    "\n",
    "    # Plot the true function values, GP mean, GP variance, and acquisition function heatmaps\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    # Plot the true function values heatmap with sample points\n",
    "    c1 = axs[0, 0].imshow(Z_true, extent=(bounds[0][0], bounds[0][1], bounds[1][0], bounds[1][1]),\n",
    "                          origin='lower', cmap='viridis', aspect='auto')\n",
    "    for i, point in enumerate(sample_x_list):\n",
    "        color = 'red' if i == len(sample_x_list) - 1 else 'blue'\n",
    "        axs[0, 0].scatter(point[0], point[1], color=color)\n",
    "    axs[0, 0].set_title(\"True Function with Sample Points\")\n",
    "    axs[0, 0].set_xlabel(\"X1\")\n",
    "    axs[0, 0].set_ylabel(\"X2\")\n",
    "    fig.colorbar(c1, ax=axs[0, 0], orientation='vertical')\n",
    "\n",
    "    # Plot the GP mean heatmap\n",
    "    c2 = axs[0, 1].imshow(Z_gp_mean, extent=(bounds[0][0], bounds[0][1], bounds[1][0], bounds[1][1]),\n",
    "                          origin='lower', cmap='coolwarm', aspect='auto')\n",
    "    axs[0, 1].set_title(\"GP Mean\")\n",
    "    axs[0, 1].set_xlabel(\"X1\")\n",
    "    axs[0, 1].set_ylabel(\"X2\")\n",
    "    fig.colorbar(c2, ax=axs[0, 1], orientation='vertical')\n",
    "\n",
    "    # Plot the GP variance heatmap\n",
    "    c3 = axs[1, 0].imshow(Z_gp_var, extent=(bounds[0][0], bounds[0][1], bounds[1][0], bounds[1][1]),\n",
    "                          origin='lower', cmap='YlGnBu', aspect='auto')\n",
    "    axs[1, 0].set_title(\"GP Variance\")\n",
    "    axs[1, 0].set_xlabel(\"X1\")\n",
    "    axs[1, 0].set_ylabel(\"X2\")\n",
    "    fig.colorbar(c3, ax=axs[1, 0], orientation='vertical')\n",
    "\n",
    "    # Plot the acquisition function heatmap\n",
    "    c4 = axs[1, 1].imshow(Z_acq, extent=(bounds[0][0], bounds[0][1], bounds[1][0], bounds[1][1]),\n",
    "                          origin='lower', cmap='inferno', aspect='auto')\n",
    "    axs[1, 1].set_title(\"Acquisition Function\")\n",
    "    axs[1, 1].set_xlabel(\"X1\")\n",
    "    axs[1, 1].set_ylabel(\"X2\")\n",
    "    fig.colorbar(c4, ax=axs[1, 1], orientation='vertical')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_true_function(true_function, bounds, title=\"True Function with Sample Points\"):\n",
    "    \"\"\"\n",
    "    Plot the true objective function values as a 2D heatmap, with sample points overlayed.\n",
    "    \"\"\"\n",
    "    # Define a grid within the bounds for plotting\n",
    "    x = np.linspace(bounds[0][0], bounds[0][1], 50)\n",
    "    y = np.linspace(bounds[1][0], bounds[1][1], 50)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    xy_grid = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "\n",
    "    # Calculate true function values over the grid\n",
    "    Z_true = np.array([true_function(point) for point in xy_grid]).reshape(X.shape)\n",
    "\n",
    "    # Plot the true function values heatmap with sample points\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    c = plt.imshow(Z_true, extent=(bounds[0][0], bounds[0][1], bounds[1][0], bounds[1][1]),\n",
    "                   origin='lower', cmap='viridis', aspect='auto')\n",
    "    plt.colorbar(c, label=\"Function Value\")\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"X1\")\n",
    "    plt.ylabel(\"X2\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19cd0a1-936f-4e41-bf21-439d75e77c3a",
   "metadata": {},
   "source": [
    "## YOUR TASKS\n",
    "\n",
    "There are four tasks within this notebook. \n",
    "- TASK 0: Complete the RBF Kernel ([Go to TASK 0](#TASK0)).\n",
    "- TASK 1: Complete the Matern Kernel (3P) ([Go to TASK 1](#TASK1)).\n",
    "- TASK 2: Improve the optimization performance (3P) ([Go to TASK 2](#TASK2)).\n",
    "- TASK 3: Complete compute_hypervolume for multi-objective optimization (3P) ([Go to TASK 3](#TASK3)).\n",
    "- TASK 4: Reflection -- where and how Bayesian optimization can be utilized to help your project make better design decisions? (1P) ([Go to TASK 4](#TASK4))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa08dbe8-57b5-4d9f-aa34-2f54c244769a",
   "metadata": {},
   "source": [
    "## 2. Basic Bayesian optimization: Gaussian Process, RBF, and EI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e96698-a934-4f5d-8688-57a3decc68c2",
   "metadata": {},
   "source": [
    "### 2.1 A simple function and setting the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7da021-33d3-414d-b41d-88ba732d35c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below, we define a function, which takes 1D input and gives 1D output\n",
    "\n",
    "def simple_test_objective(x):\n",
    "    return -(1.4 - 3.0 * x) * sin(18.0 * x)\n",
    "\n",
    "# We specify the input range as [0, 1.2]\n",
    "simple_bounds_1d = [(0, 1.2)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c715fdf5-df17-4d8f-9759-68a5d1db7b9a",
   "metadata": {},
   "source": [
    "Can you guess where the peak is within 10 trials? Any output y > 1.8 is good enough.\n",
    "\n",
    "Remember, every try needs to fall into the range of (0, 1.2). \n",
    "\n",
    "Try out in the next cell! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3519c12-bb63-466f-9308-a65cad8c9843",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "for i in range(10):\n",
    "    print(f\"Iteration: {i+1}\")\n",
    "    this_x = input(\"Enter any value (0, 1.2): \")\n",
    "    xs.append(float(this_x))\n",
    "    np_xs = np.array(xs)\n",
    "    ys = simple_test_objective(np_xs)\n",
    "    \n",
    "    plt.scatter(np_xs, ys)\n",
    "    plt.xlim(0,1.2)\n",
    "    plt.ylim(-1.7, 2.2)\n",
    "    plt.ylabel('y value', fontsize=15)\n",
    "    plt.axhline(y = 1.8, color = 'r', linestyle = '-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23451d5-a0c7-4886-baf9-e649491bff42",
   "metadata": {},
   "source": [
    "### 2.1 Gaussian Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c08fc43-68dd-4b71-9817-a4f232e589b3",
   "metadata": {},
   "source": [
    "Now, we start implementing the base functions for Bayesian optimization.\n",
    "\n",
    "There are two key elements to form Bayesian optimizaiton:\n",
    "- The surrogate model (usually a Gaussian process). Within the surrogate model, it contains (a) a mean function and (b) a kernel.\n",
    "- An acquisition function that guides the search behavior of the optimization process\n",
    "\n",
    "Here, we first impelment Gaussian process.\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09999ee2-069c-4ae1-9504-2f2772adabbe",
   "metadata": {},
   "source": [
    "### Gaussian Process Definition [IMPORTANT]\n",
    "\n",
    "A Gaussian Process (GP) is defined as a collection of random variables, any finite number of which have a joint Gaussian distribution. A GP is specified by a **mean function** $m(x)$ and a **covariance function (kernel)** $k(x, x')$:\n",
    "\n",
    "$$\n",
    "y \\sim \\mathcal{GP}(m(x), kernel(x, x'))\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $m(x)$ is the mean function, describing the expected value of $y$ at $x$.\n",
    "- $kernel(x, x')$ is the covariance function (or kernel), which quantifies the assumed similarity between $x$ and $x'$.\n",
    "\n",
    "A GP is a distribution over functions, meaning that for any set of inputs $\\{x_1, x_2, \\ldots, x_n\\}$, the corresponding outputs $\\{f(x_1), f(x_2), \\ldots, f(x_n)\\}$ follow a multivariate Gaussian distribution:\n",
    "\n",
    "$$\n",
    "\\mathbf{f} \\sim \\mathcal{N}(\\mathbf{m}, \\mathbf{K}),\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{m} = [m(x_1), m(x_2), \\ldots, m(x_n)]$ is the mean vector,\n",
    "- $\\mathbf{K}$ is the covariance matrix computed using the kernel function $k(x, x')$.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Training Covariance Matrix \n",
    "For a Gaussian Process model, the covariance matrix $ K $ for the training data is calculated as:\n",
    "\n",
    "$$\n",
    "K = kernel(X_{\\text{train}}, X_{\\text{train}}) + \\sigma_n^2 I + \\text{jitter} \\cdot I\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ K $ is the covariance matrix of the training data.\n",
    "- $ X_{\\text{train}} $ is the matrix of training inputs.\n",
    "- $ \\sigma_n^2 $ is the noise variance, accounting for observation noise.\n",
    "- The \"jitter\" term is added to ensure numerical stability when inverting $ K $ (optional, added here for stability).\n",
    "\n",
    "#### 2. Predictive Mean\n",
    "For a new test point $ X_{\\text{test}} $, the predictive mean $ \\mu_s $ is given by:\n",
    "\n",
    "$$\n",
    "\\mu_s = m(X_{\\text{test}}) + K_s^T K^{-1} y_{\\text{train}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ m(X_{\\text{test}}) $ is the mean function applied to the test points, often assumed to be zero.\n",
    "- $ K_s = \\text{kernel}(X_{\\text{train}}, X_{\\text{test}}) $ is the covariance between the training points and test points.\n",
    "- $ K^{-1} $ is the inverse of the training covariance matrix $ K $.\n",
    "- $ y_{\\text{train}} $ is the vector of centered training outputs, calculated as $ y_{\\text{train}} = Y_{\\text{train}} - m(X_{\\text{train}}) $.\n",
    "\n",
    "#### 3. Predictive Covariance (Variance)\n",
    "The predictive covariance for the test points is given by:\n",
    "\n",
    "$$\n",
    "\\Sigma_s = K_{ss} - K_s^T K^{-1} K_s\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ K_{ss} = \\text{kernel}(X_{\\text{test}}, X_{\\text{test}}) $ is the covariance between the test points.\n",
    "- The term $ K_s^T K^{-1} K_s $ adjusts the test covariance based on the correlation with the training data.\n",
    "\n",
    "#### 4. Predictive Standard Deviation\n",
    "The predictive standard deviation $ \\sigma_s $ for each test point is the square root of the diagonal elements of $ \\Sigma_s $:\n",
    "\n",
    "$$\n",
    "\\sigma_s = \\sqrt{\\text{diag}(\\Sigma_s)}\n",
    "$$\n",
    "\n",
    "This standard deviation provides an estimate of the uncertainty in the predictions at each test point.\n",
    "\n",
    "#### Summary of Predictions\n",
    "Given a test input $ X_{\\text{test}} $, the GP model predicts:\n",
    "- A **mean** $ \\mu_s $, which represents the expected value of the output.\n",
    "- A **variance** $ \\sigma_s^2 $, representing the uncertainty in the prediction. \n",
    "\n",
    "The predictive distribution for each test point $ X_{\\text{test}} $ is then:\n",
    "\n",
    "$$\n",
    "f(X_{\\text{test}}) \\sim \\mathcal{N}(\\mu_s, \\sigma_s^2)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\mu_s $ is the predicted mean.\n",
    "- $ \\sigma_s^2 $ is the predicted variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c52e4d-2513-43ec-bd8e-fdf2685b3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProcess:\n",
    "    def __init__(self, kernel, mean_func=lambda x: 0, sigma_n=1e-8):\n",
    "        # In this implementation, the user inits the Gaussian Process by specifying a kernel, mean function, and noise variance\n",
    "        self.kernel = kernel                # Kernel function, e.g., RBF, defining similarity between points\n",
    "        self.mean_func = mean_func          # Mean function, often set to zero by default\n",
    "        self.sigma_n = sigma_n              # Observation noise variance\n",
    "        self.jitter = 1e-6                  # Small jitter term for numerical stability in matrix inversion\n",
    "        self.X_train = None                 # Placeholder for training input data\n",
    "        self.y_train = None                 # Placeholder for training output data\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        # Fit the GP model to training data\n",
    "        self.X_train = X_train\n",
    "        # Center training outputs by subtracting the mean function value at each training point\n",
    "        self.y_train = y_train - self.mean_func(X_train)\n",
    "        # Compute the covariance (kernel) matrix K for the training data and add noise + jitter to the diagonal\n",
    "        self.K = self.kernel(X_train, X_train) + (self.sigma_n**2 + self.jitter) * np.eye(len(X_train))\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Predict mean and covariance for new input points X_test\n",
    "\n",
    "        # Compute cross-covariance between training data and test points\n",
    "        K_s = self.kernel(self.X_train, X_test)\n",
    "        # Compute covariance between test points\n",
    "        K_ss = self.kernel(X_test, X_test)\n",
    "        # Compute the inverse of the training covariance matrix\n",
    "        K_inv = np.linalg.inv(self.K)\n",
    "\n",
    "        # Compute the predictive mean by combining the mean function and the weighted training outputs\n",
    "        mu_s = self.mean_func(X_test) + np.dot(K_s.T, np.dot(K_inv, self.y_train.reshape(-1, 1))).flatten()\n",
    "\n",
    "        # Compute the predictive covariance\n",
    "        cov_s = K_ss - np.dot(K_s.T, np.dot(K_inv, K_s))\n",
    "\n",
    "        # Return the predictive mean and the standard deviation (square root of diagonal of covariance)\n",
    "        return mu_s, np.sqrt(np.diag(cov_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457aa5c8-5ed8-4787-85ee-e126e25431cc",
   "metadata": {},
   "source": [
    "### 2.2 Radial Basis Function (RBF) Kernel\n",
    "<a id=\"TASK0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6837081-ccd2-465e-ac1f-34e2cf9780ad",
   "metadata": {},
   "source": [
    "The **RBF kernel** (also known as the Exponentiated Quadratic kernel or Gaussian kernel) is defined as follows:\n",
    "\n",
    "$$\n",
    "k(X_1, X_2) = \\sigma^2 \\exp \\left( -\\frac{\\|X_1 - X_2\\|^2}{2 \\ell^2} \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $X_1$ and $X_2$ are two sets of points (or vectors),\n",
    "- $\\sigma^2$ is the **signal variance** or **output scale**, which controls the overall magnitude of the kernel function,\n",
    "- $\\ell$ is the **length scale** parameter, controlling how quickly the function varies with distance between points,\n",
    "- $\\|X_1 - X_2\\|^2$ is the **squared Euclidean distance** between $X_1$ and $X_2$.\n",
    "\n",
    "---\n",
    "\n",
    "### Squared Euclidean Distance Calculation [IMPORTANT]\n",
    "\n",
    "The squared Euclidean distance between points in $X_1$ and $X_2$ is calculated as:\n",
    "\n",
    "$$\n",
    "\\|X_1 - X_2\\|^2 = \\sum_{i} X_{1i}^2 + \\sum_{j} X_{2j}^2 - 2 X_1 \\cdot X_2^T\n",
    "$$\n",
    "\n",
    "where:\n",
    "- The term $\\sum_{i} X_{1i}^2$ calculates the sum of squared elements in each row of $X_1$ (resulting in a column vector),\n",
    "- The term $\\sum_{j} X_{2j}^2$ calculates the sum of squared elements in each row of $X_2$ (resulting in a row vector),\n",
    "- The term $-2 X_1 \\cdot X_2^T$ calculates the pairwise product of points in $X_1$ and $X_2$, which is subtracted to complete the distance calculation.\n",
    "\n",
    "This formulation of the RBF kernel quantifies the similarity between points in input space. Points that are closer together (smaller $\\|X_1 - X_2\\|^2$) have a higher kernel value, indicating greater similarity. Conversely, points farther apart have lower kernel values, reflecting reduced similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7557c06-5336-4225-9df3-62ebae756abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RBF kernel function\n",
    "def rbf_kernel(X1, X2, length_scale=0.1, sigma_f=1.0):\n",
    "    X1 = np.atleast_2d(X1)\n",
    "    X2 = np.atleast_2d(X2)\n",
    "    # Compute the squared Euclidean distance matrix\n",
    "    sqdist = np.sum(X1**2, axis=1).reshape(-1, 1) + np.sum(X2**2, axis=1) - 2 * np.dot(X1, X2.T)\n",
    "    return 0 # FIX THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7b08b-ec88-4a74-ab96-1339e0cfcfdb",
   "metadata": {},
   "source": [
    "### 2.3 Expected Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a238984-3117-442c-90a5-0bd33ef011dc",
   "metadata": {},
   "source": [
    "\n",
    "The Expected Improvement (EI) acquisition function is used in Bayesian optimization to balance exploration and exploitation when selecting new points for evaluation. EI provides the expected value of improvement over the best observed objective value, $y_{\\text{best}}$.\n",
    "\n",
    "#### Formula for Expected Improvement\n",
    "\n",
    "For a candidate point $x$ and a Gaussian Process model prediction at $x$ with mean $\\mu(x)$ and standard deviation $\\sigma(x)$, the Expected Improvement is defined as:\n",
    "\n",
    "$$\n",
    "\\text{EI}(x) = \\mathbb{E}\\left[\\max(0, y - y_{\\text{best}}) \\mid \\mathcal{D}\\right],\n",
    "$$\n",
    "\n",
    "where the expectation is taken over the posterior predictive distribution of $y$ at $x$, conditioned on the observed data $\\mathcal{D}$). This can be expressed in a computable form when $y$ is assumed to follow a Gaussian distribution:\n",
    "\n",
    "$$\n",
    "\\text{EI}(x) = (\\mu(x) - y_{\\text{best}} + \\xi) \\cdot \\Phi(Z) + \\sigma(x) \\cdot \\phi(Z),\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mu(X)$ is the predictive mean at $X$ from the Gaussian Process,\n",
    "- $\\sigma(x)$ is the predictive standard deviation at \\(X\\),\n",
    "- $y_{\\text{best}}$ is the best observed value so far,\n",
    "- $\\xi$ is a small, positive exploration parameter that promotes exploration,\n",
    "- $Z = \\frac{\\mu(x) - y_{\\text{best}} - \\xi}{\\sigma(x)}$ is the standardized improvement,\n",
    "- $\\Phi(Z)$ is the cumulative distribution function (CDF) of the standard normal distribution,\n",
    "- $\\phi(Z)$ is the probability density function (PDF) of the standard normal distribution.\n",
    "\n",
    "#### Question for yourself later\n",
    "\n",
    "What exactly do $\\Phi(Z)$ and $\\phi(Z)$ represent? What are their purpose?\n",
    "\n",
    "\n",
    "#### Components of EI\n",
    "\n",
    "1. **Improvement**:\n",
    "   The improvement term measures the difference between the predicted mean $\\mu(x)$ and the best observed value $y_{\\text{best}}$, adjusted by $\\xi$:\n",
    "\n",
    "   $$\n",
    "   \\text{improvement} = \\mu(x) - y_{\\text{best}} - \\xi\n",
    "   $$\n",
    "\n",
    "2. **Standardized Improvement (Z-score)**:\n",
    "   To make the improvement term dimensionless and comparable across different values of $\\sigma(x)$, we standardize it by dividing by $\\sigma(X)$:\n",
    "\n",
    "   $$\n",
    "   Z = \\frac{\\text{improvement}}{\\sigma(x)}\n",
    "   $$\n",
    "\n",
    "3. **Probability and Density Functions**:\n",
    "   - $\\Phi(Z)$ is the cumulative distribution function (CDF) of the standard normal distribution, giving the probability that a normally distributed random variable with mean $\\mu(x)$ and standard deviation $\\sigma(X)$ exceeds $y_{\\text{best}}$ (+ $\\xi$).\n",
    "   - $\\phi(Z)$ is the probability density function (PDF) of the standard normal distribution, providing the density at $Z$.\n",
    "\n",
    "#### Putting It All Together\n",
    "\n",
    "Combining these elements, EI represents the expected value of improvement over $y_{\\text{best}}$, balancing exploitation (focusing on areas with high predictive mean) and exploration (considering areas with high uncertainty, i.e., high $\\sigma(X)$). The final Expected Improvement formula is:\n",
    "\n",
    "$$\n",
    "\\text{EI}(x) = (\\mu(x) - y_{\\text{best}} - \\xi) \\cdot \\Phi(Z) + \\sigma(x) \\cdot \\phi(Z)\n",
    "$$\n",
    "\n",
    "where $\\Phi(Z)$ and $\\phi(Z)$ are applied to the standardized improvement $Z = \\frac{\\text{improvement}}{\\sigma(x)}$.\n",
    "\n",
    "#### Summary\n",
    "- **Exploration** is promoted by considering $\\sigma(x)$, which adds value in regions where the GP model has high uncertainty.\n",
    "- **Exploitation** is promoted by $\\mu(x) - y_{\\text{best}}$, which favors regions where the GP predicts high values.\n",
    "- Balancing between **Exploration** and **Exploitation**: The **Z-score** ($Z$) dynamically balances exploration and exploitation by standardizing the improvement term with respect to uncertainty $\\sigma(x)$. Larger $Z$ values indicate regions where predicted improvement is more likely and significant, while higher uncertainty (small $Z$) encourages exploration in less-explored areas.\n",
    "\n",
    "\n",
    "The EI acquisition function is an effective way to guide the search toward regions that balance the potential for improvement and uncertainty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a82502-b157-409e-97b5-493d114f4f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_improvement(X, gp, y_best, xi=0.01):\n",
    "    \"\"\"\n",
    "    Compute the Expected Improvement (EI) acquisition function values.\n",
    "\n",
    "    Parameters:\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        The input samples where the acquisition function is evaluated.\n",
    "    gp : GaussianProcess object\n",
    "        A fitted Gaussian Process model that provides predictions.\n",
    "    y_best : float\n",
    "        The best observed objective function value.\n",
    "    xi : float\n",
    "        Exploration parameter, encouraging more exploration if higher.\n",
    "\n",
    "    Returns:\n",
    "    ei : array-like, shape (n_samples,)\n",
    "        Expected Improvement values for each sample in X.\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.atleast_2d(X)\n",
    "    mu, std = gp.predict(X)\n",
    "    epsilon = 1e-9\n",
    "    std = np.maximum(std, epsilon)  # Prevent division by zero\n",
    "    improvement = mu - y_best - xi\n",
    "    Z = improvement / std\n",
    "    ei = improvement * norm.cdf(Z) + std * norm.pdf(Z)\n",
    "    return ei[0] if ei.size == 1 else ei  # Return scalar for single point, array otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88441431-9844-45d7-ab0e-21843ffb9e57",
   "metadata": {},
   "source": [
    "### 2.5 Visualize GP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae7cb10-97e3-4d42-87b7-e193e5e2bd71",
   "metadata": {},
   "source": [
    "Let's first visualize the GP model, and see if it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7506aca-2517-429c-b0c9-6796dbb079f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample initial points and their corresponding function values\n",
    "\n",
    "sample_x_list = np.array([0.0, 0.5, 1.0])\n",
    "sample_y_list = simple_test_objective(sample_x_list)\n",
    "\n",
    "# Initialize and fit the custom GP model using the sample data\n",
    "gp_custom = GaussianProcess(kernel=rbf_kernel)\n",
    "gp_custom.fit(sample_x_list.reshape(-1, 1), sample_y_list)\n",
    "\n",
    "# Define sample points for plotting\n",
    "sample_points = np.arange(0, 1.2, 0.01)\n",
    "\n",
    "# Define our best observed value so far\n",
    "y_best = np.max(sample_y_list)\n",
    "print(y_best)\n",
    "\n",
    "# Compute the EI values for each point in the sample_points using the custom GP model\n",
    "ei_values = expected_improvement(\n",
    "    X=sample_points.reshape(-1, 1),  # Points where we evaluate EI\n",
    "    gp=gp_custom,\n",
    "    y_best=y_best,\n",
    "    xi=0.01\n",
    ")\n",
    "\n",
    "# Plot the GP, true function, and EI acquisition function\n",
    "plot_gp_and_acquisition(\n",
    "    true_function=simple_test_objective,\n",
    "    gp_model=gp_custom,\n",
    "    sample_x_list=sample_x_list,\n",
    "    sample_y_list=sample_y_list,\n",
    "    sample_points=sample_points,\n",
    "    acquisition_values=ei_values,\n",
    "    acq_x_range=sample_points\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5553094-d41a-4ef7-b000-e1f26846268a",
   "metadata": {},
   "source": [
    "### 2.6 Putting everything together [IMPORTANT]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0b7109-2826-4cfb-b81b-0cd979fc5e59",
   "metadata": {},
   "source": [
    "Psuedo code for Bayesian optimization\n",
    "\n",
    "1. **Initialize and Sample Initial Points**\n",
    "    - Determine the problem dimensionality based on `bounds`.\n",
    "    - Randomly sample `num_random_init` points within `bounds` as initial observations.\n",
    "    - Evaluate the `target_function` at each initial point to get corresponding output values.\n",
    "\n",
    "2. **Initialize Gaussian Process Model**\n",
    "    - Create and fit a Gaussian Process (GP) model to the initial observed data using the specified `kernel` and `mean_func`.\n",
    "\n",
    "3. **Optimization Loop** (for each step in `num_steps`):\n",
    "    - **Generate Candidate Points** within the specified bounds to evaluate potential next points.\n",
    "    \n",
    "    - **Compute Acquisition Function** values for each candidate point using the GP model. This guides the search toward regions that maximize improvement over the best-observed value.\n",
    "    \n",
    "    - **Select Next Point** as the candidate with the highest acquisition function value and evaluate `target_function` at this point.\n",
    "\n",
    "    - **Update Observations and GP Model** by adding the new point and outcome, then refitting the GP model on the updated dataset.\n",
    "\n",
    "4. **Return Results**\n",
    "    - After completing all steps, return the observed points, their function values, and the final GP model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db5b175-82e1-4351-ad1f-c1d7616bb37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_optimization(target_function, bounds, num_steps, num_random_init=5, kernel=rbf_kernel, acquisition_func=\"EI\", xi=0.01, kappa=2.0, mean_func=lambda x: 0, plot_progress=True):\n",
    "    is_1d = len(bounds) == 1\n",
    "    num_dims = 1 if is_1d else len(bounds)\n",
    "    \n",
    "    # Generate initial random samples within the specified bounds\n",
    "    X_observed = np.random.uniform(\n",
    "        [b[0] for b in bounds], [b[1] for b in bounds], size=(num_random_init, num_dims)\n",
    "    )\n",
    "    y_observed = np.array([target_function(x if not is_1d else x[0]) for x in X_observed]).flatten()\n",
    "\n",
    "    # Initialize the GP model with the random samples and custom mean function\n",
    "    gp = GaussianProcess(kernel=kernel, mean_func=mean_func)\n",
    "    gp.fit(X_observed, y_observed)\n",
    "\n",
    "    # Begin optimization loop\n",
    "    for step in range(num_steps):\n",
    "        # Generate a grid within the bounds\n",
    "        grids = [np.linspace(b[0], b[1], 50) for b in bounds]  # Adjust grid density as needed\n",
    "        X_grid = np.array(np.meshgrid(*grids)).T.reshape(-1, num_dims)\n",
    "\n",
    "        # Select acquisition function\n",
    "        if acquisition_func == \"EI\":\n",
    "            af_values = expected_improvement(X_grid, gp, np.max(y_observed), xi=xi)\n",
    "        elif acquisition_func == \"UCB\":\n",
    "            af_values = upper_confidence_bound(X_grid, gp, kappa=kappa)\n",
    "        elif acquisition_func == \"PI\":\n",
    "            af_values = probability_of_improvement(X_grid, gp, np.max(y_observed), xi=xi)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown acquisition function. Choose from 'EI', 'UCB', 'PI'.\")\n",
    "\n",
    "        # Select next point as the arg max of the acquisition function\n",
    "        next_x = X_grid[np.argmax(af_values)].reshape(1, -1)\n",
    "        next_y = target_function(next_x.flatten() if not is_1d else next_x[0, 0])\n",
    "\n",
    "        # Add the new observation\n",
    "        X_observed = np.vstack((X_observed, next_x))\n",
    "        y_observed = np.append(y_observed, next_y)\n",
    "\n",
    "        # Update the GP model with the new data\n",
    "        gp.fit(X_observed, y_observed)\n",
    "\n",
    "        # Print step results\n",
    "        print(f\"Step {step + 1} ({acquisition_func}): x={next_x.flatten() if not is_1d else next_x[0, 0]}, y={next_y}\")\n",
    "        \n",
    "        # Plot results based on dimensionality, if plot_progress is True\n",
    "        if plot_progress:\n",
    "            if is_1d:\n",
    "                plot_gp_and_acquisition(\n",
    "                    true_function=target_function,\n",
    "                    gp_model=gp,\n",
    "                    sample_x_list=X_observed.flatten(),\n",
    "                    sample_y_list=y_observed,\n",
    "                    sample_points=X_grid.flatten(),\n",
    "                    acquisition_values=af_values,\n",
    "                    acq_x_range=X_grid.flatten()\n",
    "                )\n",
    "            else:\n",
    "                plot_2d_gp_and_acquisition(\n",
    "                    true_function=target_function,\n",
    "                    gp_model=gp,\n",
    "                    sample_x_list=X_observed,\n",
    "                    sample_y_list=y_observed,\n",
    "                    bounds=bounds,\n",
    "                    acquisition_function=expected_improvement,\n",
    "                    xi=xi\n",
    "                )\n",
    "\n",
    "    return X_observed, y_observed, gp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d9a6fb-afa3-4398-bdd5-dd49d8c819ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Bayesian Optimization with bounds [0, 1.2], 5 random initial samples, and 10 total steps\n",
    "X_observed, y_observed, gp_model = bayesian_optimization(\n",
    "        target_function=simple_test_objective,\n",
    "        bounds=simple_bounds_1d,\n",
    "        num_steps=5,  # Reduced steps for quick comparison\n",
    "        num_random_init=5,\n",
    "        acquisition_func=\"EI\",\n",
    "        xi=0.01,\n",
    "        kappa=2.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa57d9aa-2428-4e5f-b41e-ee745ee4395f",
   "metadata": {},
   "source": [
    "## 3. More kernels, acquisition functions, and 2D functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd516de-c8ad-497a-b02e-84ee322c4d28",
   "metadata": {},
   "source": [
    "### 3.1 More kernels \n",
    "\n",
    "[IMPORTANT for MATERN KERNEL, PEDIODIC KERNEL; DON'T NEED TO MEMERIZE THE MATH, BUT YOU NEED TO KNOW THE CONCEPTUAL DIFFERENCES]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44614704-6b08-4a17-9a6b-7909dec256e5",
   "metadata": {},
   "source": [
    "#### 0. Length Scale: A Key Parameter in most Kernel Functions\n",
    "\n",
    "The **length scale** parameter, denoted as $\\text{length\\_scale}$, controls the smoothness of the function generated by the Gaussian Process. It dictates how quickly the correlation between points decreases as they move further apart.\n",
    "\n",
    "- **Small** $\\text{length\\_scale}$ values cause the model to vary rapidly, fitting closely to nearby points and capturing fine-grained patterns.\n",
    "- **Large** $\\text{length\\_scale}$ values lead to smoother functions, where points far apart still maintain high correlation.\n",
    "\n",
    "This parameter affects each kernel's behavior, influencing the resulting function's flexibility and generalization.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Matern Kernel (YOUR TASK 1, 3P) \n",
    "<a id=\"TASK1\"></a>\n",
    "\n",
    "\n",
    "The Matérn kernel is a popular choice in Gaussian Process modeling because it provides a tunable smoothness parameter, $\\nu$, which controls the smoothness of the resulting function. The general form of the Matérn kernel is:\n",
    "\n",
    "$$\n",
    "K(X_i, X_j) = \\sigma_f^2 \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left( \\frac{\\sqrt{2\\nu} \\cdot d_{ij}}{\\text{length\\_scale}} \\right)^\\nu K_\\nu \\left( \\frac{\\sqrt{2\\nu} \\cdot d_{ij}}{\\text{length\\_scale}} \\right),\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ d_{ij} $ is the Euclidean distance between points $X_i$ and $X_j$,\n",
    "- $\\sigma_f^2$ is the output variance, controlling the amplitude of the kernel,\n",
    "- $\\text{length\\_scale}$ determines how quickly correlations decay with distance,\n",
    "- $\\Gamma(\\nu)$ is the Gamma function,\n",
    "- $K_\\nu$ is the modified Bessel function of the second kind.\n",
    "\n",
    "For specific values of $\\nu = 0.5, 1.5,$ and $2.5$, the kernel function simplifies to more practical forms:\n",
    "\n",
    "Let $d_{ij}$ be the Euclidean distance between points $X_i$ and $X_j$:\n",
    "$$\n",
    "d_{ij} = \\sqrt{\\sum_{k=1}^{n} (X_{i} - X_{j})^2}.\n",
    "$$\n",
    "\n",
    "The Matérn kernel is given by:\n",
    "\n",
    "- **When** $\\nu = 0.5$ (exponential kernel):\n",
    "  $$\n",
    "  K(X_i, X_j) = \\sigma_f^2 \\exp\\left(-\\frac{d_{ij}}{\\text{length\\_scale}}\\right)\n",
    "  $$\n",
    "\n",
    "- **When** $\\nu = 1.5$:\n",
    "  $$\n",
    "  K(X_i, X_j) = \\sigma_f^2 \\left(1 + \\frac{\\sqrt{3} \\cdot d_{ij}}{\\text{length\\_scale}}\\right) \\exp\\left(-\\frac{\\sqrt{3} \\cdot d_{ij}}{\\text{length\\_scale}}\\right)\n",
    "  $$\n",
    "\n",
    "- **When** $\\nu = 2.5$:\n",
    "  $$\n",
    "  K(X_i, X_j) = \\sigma_f^2 \\left(1 + \\frac{\\sqrt{5} \\cdot d_{ij}}{\\text{length\\_scale}} + \\frac{5 \\cdot d_{ij}^2}{3 \\cdot \\text{length\\_scale}^2}\\right) \\exp\\left(-\\frac{\\sqrt{5} \\cdot d_{ij}}{\\text{length\\_scale}}\\right)\n",
    "$$\n",
    "\n",
    "where $\\sigma_f^2$ is the output variance, controlling the amplitude of the kernel.\n",
    "\n",
    "**What is the key difference between Matern Kernel and RBF Kernel?** \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Periodic Kernel\n",
    "\n",
    "The periodic kernel is useful for capturing repeating patterns and is defined by the following expression:\n",
    "\n",
    "$$ K(X_i, X_j) = \\sigma_f^2 \\exp\\left(-\\frac{2 \\sin^2\\left(\\frac{\\pi d_{ij}}{\\text{period}}\\right)}{\\text{length\\_scale}^2}\\right) $$\n",
    "\n",
    "where:\n",
    "- $ d_{ij} = \\| X_i - X_j \\| $ is the Euclidean distance.\n",
    "- $ \\text{period} $ defines the distance between repetitions of the pattern.\n",
    "- $ \\text{length\\_scale} $ controls the smoothness of the periodicity.\n",
    "\n",
    "**What is the key difference between Periodic Kernel and RBF Kernel?**\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Rational Quadratic Kernel\n",
    "\n",
    "The Rational Quadratic kernel is equivalent to a weighted sum of RBF kernels with varying length scales and is defined as:\n",
    "\n",
    "$$ K(X_i, X_j) = \\sigma_f^2 \\left(1 + \\frac{d_{ij}^2}{2 \\alpha \\cdot \\text{length\\_scale}^2}\\right)^{-\\alpha} $$\n",
    "\n",
    "where:\n",
    "- $ d_{ij} = \\| X_i - X_j \\|^2 $ is the squared Euclidean distance.\n",
    "- $ \\alpha $ is a parameter that controls the relative weighting of different length scales.\n",
    "- $ \\sigma_f^2 $ is the output variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a30d310-ac95-4e6d-8832-0ec4d19af387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define additional kernel functions\n",
    "\n",
    "# Matern Kernel\n",
    "def matern_kernel(X1, X2, length_scale=1.0, nu=2.5, sigma_f=1.0):\n",
    "    \"\"\"\n",
    "    Matern kernel with parameter nu (1.5 or 2.5 are typical values).\n",
    "    \"\"\"\n",
    "    dists = np.sqrt(np.sum((X1[:, None] - X2[None, :])**2, axis=2))\n",
    "    if nu == 0.5:\n",
    "        K = sigma_f**2 * np.exp(-dists / length_scale)  ## THIS LINE IS CORRECT, DO NOT CHANGE!\n",
    "    elif nu == 1.5:\n",
    "        K = sigma_f**2 * np.exp(-dists / length_scale)  ## CORRECT THIS ONE\n",
    "    elif nu == 2.5:\n",
    "        K = sigma_f**2 * np.exp(-dists / length_scale)  ## CORRECT THIS ONE\n",
    "    return K\n",
    "\n",
    "# Periodic Kernel\n",
    "def periodic_kernel(X1, X2, length_scale=1.0, period=1.0, sigma_f=1.0):\n",
    "    \"\"\"\n",
    "    Periodic kernel for capturing repeating patterns.\n",
    "    \"\"\"\n",
    "    dists = np.sqrt(np.sum((X1[:, None] - X2[None, :])**2, axis=2))\n",
    "    return sigma_f**2 * np.exp(-2 * (np.sin(np.pi * dists / period)**2) / length_scale**2)\n",
    "\n",
    "# Rational Quadratic Kernel\n",
    "def rational_quadratic_kernel(X1, X2, length_scale=1.0, alpha=1.0, sigma_f=1.0):\n",
    "    \"\"\"\n",
    "    Rational Quadratic kernel, similar to a mixture of RBF kernels with different length scales.\n",
    "    \"\"\"\n",
    "    dists = np.sum((X1[:, None] - X2[None, :])**2, axis=2)\n",
    "    return sigma_f**2 * (1 + dists / (2 * alpha * length_scale**2))**(-alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acde069-e07c-473c-bf88-0f7e062099b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the difference between RBF and Matern\n",
    "# Generate sample points\n",
    "\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "# Compute kernel values using the provided functions\n",
    "K_matern_05 = matern_kernel(X, X, length_scale=1.0, nu=0.5)\n",
    "K_matern_15 = matern_kernel(X, X, length_scale=1.0, nu=1.5)\n",
    "K_matern_25 = matern_kernel(X, X, length_scale=1.0, nu=2.5)\n",
    "K_rbf = rbf_kernel(X, X, length_scale=1.0)\n",
    "\n",
    "# Plot the kernels\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(X, K_matern_05[:, 50], label=r'Matérn $\\nu=0.5$ (rough)', linestyle='--', linewidth=2)\n",
    "plt.plot(X, K_matern_15[:, 50], label=r'Matérn $\\nu=1.5$ (moderate smoothness)', linestyle='-.', linewidth=2)\n",
    "plt.plot(X, K_matern_25[:, 50], label=r'Matérn $\\nu=2.5$ (smooth)', linestyle='-', linewidth=2)\n",
    "plt.plot(X, K_rbf[:, 50], label='RBF (infinitely smooth)', linestyle=':', linewidth=2)\n",
    "\n",
    "# Add labels, legend, and grid\n",
    "plt.title('Comparison of Matérn Kernel (Different $\\nu$) and RBF Kernel', fontsize=16)\n",
    "plt.xlabel('Distance from center point', fontsize=14)\n",
    "plt.ylabel('Kernel Value (Similarity)', fontsize=14)\n",
    "plt.legend(fontsize=8)\n",
    "plt.grid(alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890ac8b1-82c5-4eff-b01b-d28050a5fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to run BO with different kernels\n",
    "def run_bo_with_kernel(kernel, kernel_name):\n",
    "    print(f\"\\nRunning Bayesian Optimization with {kernel_name}...\\n\")\n",
    "    X_observed, y_observed, gp_model = bayesian_optimization(\n",
    "        target_function=simple_test_objective,\n",
    "        bounds=simple_bounds_1d,\n",
    "        num_steps=5,  # Reduced steps for comparison\n",
    "        num_random_init=5,\n",
    "        kernel=kernel,  # Pass the specified kernel\n",
    "        xi=0.01\n",
    "    )\n",
    "    return X_observed, y_observed, gp_model\n",
    "\n",
    "# Run Bayesian Optimization with each kernel\n",
    "results = {}\n",
    "kernels = {\n",
    "    \"RBF Kernel\": rbf_kernel,\n",
    "    \"Matern Kernel\": lambda X1, X2: matern_kernel(X1, X2, nu=1.5),\n",
    "    \"Periodic Kernel\": periodic_kernel,\n",
    "    \"Rational Quadratic Kernel\": rational_quadratic_kernel\n",
    "}\n",
    "\n",
    "# Test each kernel\n",
    "for kernel_name, kernel in kernels.items():\n",
    "    results[kernel_name] = run_bo_with_kernel(kernel, kernel_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2df019-85a4-4aaf-8720-9a8fbbe99af2",
   "metadata": {},
   "source": [
    "### 3.2 More acquisition functions\n",
    "\n",
    "[IMPORTANT; NO NEED TO MEMERIZE THE MATH, BUT NEED TO KNOW THE CONCEPTUAL DIFFERENCE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa475aa-dd59-49f5-9e00-8a6c39290301",
   "metadata": {},
   "source": [
    "#### 1. Upper Confidence Bound (UCB) Acquisition Function\n",
    "\n",
    "The Upper Confidence Bound (UCB) acquisition function encourages exploration by considering both the predicted mean and uncertainty in the model. It is defined as:\n",
    "\n",
    "$$\n",
    "\\text{UCB}(X) = \\mu(X) + \\kappa \\cdot \\sigma(X)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ X $ represents the candidate points to evaluate.\n",
    "- $ \\mu(X) $ is the **predicted mean** at $ X $, which reflects the Gaussian Process's best guess at the objective function value.\n",
    "- $ \\sigma(X) $ is the **predicted standard deviation** at $ X $, representing the uncertainty in the prediction.\n",
    "- $ \\kappa $ is a trade-off parameter that controls the balance between exploration and exploitation:\n",
    "  - A **higher** $ \\kappa $ value places more emphasis on $ \\sigma(X) $, encouraging exploration by prioritizing points with high uncertainty.\n",
    "  - A **lower** $ \\kappa $ value places more emphasis on $ \\mu(X) $, favoring points expected to have high values, thus focusing on exploitation.\n",
    "\n",
    "The UCB acquisition function is particularly useful in high-dimensional spaces because it provides a simple balance between exploring unknown areas and exploiting promising ones.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Probability of Improvement (PI) Acquisition Function\n",
    "\n",
    "The Probability of Improvement (PI) acquisition function selects points with a high probability of outperforming the best observed value. The PI function is defined as:\n",
    "\n",
    "$$\n",
    "\\text{PI}(X) = \\Phi\\left( \\frac{\\mu(X) - y_{\\text{best}} - \\xi}{\\sigma(X)} \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\Phi $ represents the **cumulative distribution function** (CDF) of the standard normal distribution.\n",
    "- $ X $ is the candidate point to evaluate.\n",
    "- $ \\mu(X) $ is the **predicted mean** at $ X $.\n",
    "- $ y_{\\text{best}} $ is the **best observed value** so far, representing the highest (or lowest in minimization) sampled objective function value.\n",
    "- $ \\sigma(X) $ is the **predicted standard deviation** at $ X $.\n",
    "- $ \\xi $ is an **exploration parameter** that encourages more exploration when set to a positive value:\n",
    "  - A **higher** $ \\xi $ allows the acquisition function to consider more diverse points, increasing exploration.\n",
    "  - A **lower** $ \\xi $ makes the acquisition function more exploitative, focusing on points with high expected improvements.\n",
    "\n",
    "The PI function evaluates the probability that the sample at $ X $ will improve upon $ y_{\\text{best}} $ by at least $ \\xi $. It is useful when the goal is to quickly locate regions with high values rather than fully exploring the entire space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fd689d-17df-4222-9bbe-b5f444122cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper Confidence Bound (UCB) Acquisition Function\n",
    "\n",
    "def upper_confidence_bound(X, gp, kappa=2.0):\n",
    "    \"\"\"\n",
    "    Compute the Upper Confidence Bound (UCB) acquisition function values.\n",
    "    Parameters:\n",
    "    - X : array-like, shape (n_samples, n_features)\n",
    "        The input samples where the acquisition function is evaluated.\n",
    "    - gp : GaussianProcess object\n",
    "        A fitted Gaussian Process model that provides predictions.\n",
    "    - kappa : float\n",
    "        Controls the trade-off between exploration and exploitation (higher values = more exploration).\n",
    "        \n",
    "    Returns:\n",
    "    - ucb : array-like, shape (n_samples,)\n",
    "        Upper Confidence Bound values for each sample in X.\n",
    "    \"\"\"\n",
    "\n",
    "    mu, std = gp.predict(X)\n",
    "    ucb = mu + kappa * std\n",
    "    return ucb\n",
    "\n",
    "# Probability of Improvement (PI) Acquisition Function\n",
    "def probability_of_improvement(X, gp, y_best, xi=0.01):\n",
    "    \"\"\"\n",
    "    Compute the Probability of Improvement (PI) acquisition function values.\n",
    "\n",
    "    Parameters:\n",
    "    - X : array-like, shape (n_samples, n_features)\n",
    "        The input samples where the acquisition function is evaluated.\n",
    "    - gp : GaussianProcess object\n",
    "        A fitted Gaussian Process model that provides predictions.\n",
    "    - y_best : float\n",
    "        The best observed objective function value.\n",
    "    - xi : float\n",
    "        Exploration parameter, encouraging more exploration if higher.\n",
    "\n",
    "    Returns:\n",
    "    - pi : array-like, shape (n_samples,)\n",
    "        Probability of Improvement values for each sample in X.\n",
    "    \"\"\"\n",
    "    mu, std = gp.predict(X)\n",
    "    Z = (mu - y_best - xi) / std\n",
    "    pi = norm.cdf(Z)\n",
    "    return pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d71f3-1d36-4998-8af5-5be60d8048c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Bayesian Optimization with different acquisition functions\n",
    "acquisition_functions = [\"EI\", \"UCB\", \"PI\"]\n",
    "results_af = {}\n",
    "\n",
    "for af in acquisition_functions:\n",
    "    print(f\"\\nRunning Bayesian Optimization with {af} acquisition function...\\n\")\n",
    "    results_af[af] = bayesian_optimization(\n",
    "        target_function=simple_test_objective,\n",
    "        bounds=simple_bounds_1d,\n",
    "        num_steps=5,  # Reduced steps for quick comparison\n",
    "        num_random_init=5,\n",
    "        acquisition_func=af,\n",
    "        xi=0.01,\n",
    "        kappa=2.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9174192-0cc1-414b-bcc0-31d5986f50f2",
   "metadata": {},
   "source": [
    "### 3.3 Bayesian optimization handling multi-dimensional input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae73fc9e-b5cf-4aa2-a1e3-29a865ab8755",
   "metadata": {},
   "source": [
    "We now move on to a more challenging scenario: the function has 2D input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba2385-b13d-4b4e-b41e-0b00ad349979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branin is one of the most common function for testing optimization methods\n",
    "\n",
    "bounds_branin = [(-5, 10), (0, 15)]\n",
    "def branin(x):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - x : array-like, [x1, x2], where x1 and x2 are the normalized input coordinates in [0, 1]^2.\n",
    "    \n",
    "    Returns:\n",
    "    - value : float, the function value at the specified coordinates in the typical Branin range.\n",
    "    \"\"\"\n",
    "    x1, x2 = x\n",
    "    # Branin function calculation\n",
    "    a = 1.0\n",
    "    b = 5.1 / (4 * np.pi**2)\n",
    "    c = 5 / np.pi\n",
    "    r = 6\n",
    "    s = 10\n",
    "    t = 1 / (8 * np.pi)\n",
    "    return 5 - (a * (x2 - b * x1**2 + c * x1 - r)**2 + s * (1 - t) * np.cos(x1) + s) / 10\n",
    "\n",
    "plot_true_function(branin, bounds_branin, title=\"True Function\")\n",
    "\n",
    "# Define the grid resolution\n",
    "grid_resolution = 100  # Adjust for finer or coarser grid\n",
    "\n",
    "# Define the grid over the bounds\n",
    "x1_vals = np.linspace(bounds_branin[0][0], bounds_branin[0][1], grid_resolution)\n",
    "x2_vals = np.linspace(bounds_branin[1][0], bounds_branin[1][1], grid_resolution)\n",
    "X1, X2 = np.meshgrid(x1_vals, x2_vals)\n",
    "grid_points = np.c_[X1.ravel(), X2.ravel()]\n",
    "\n",
    "# Evaluate the Branin function on the grid\n",
    "branin_values = np.array([branin(point) for point in grid_points])\n",
    "\n",
    "# Find the maximum Branin function value\n",
    "max_branin_value = np.max(branin_values)\n",
    "max_branin_point = grid_points[np.argmax(branin_values)]\n",
    "\n",
    "print(f\"Maximum Branin function value: {max_branin_value}\")\n",
    "print(f\"At point: {max_branin_point}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af617104-b703-4656-a0b5-67af7a18e5f7",
   "metadata": {},
   "source": [
    "Below, we show our 2D visualization function, which will be useful for you to complete the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d755fd10-1795-48f4-92ae-382f8fe61dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 5 points within the bounds\n",
    "sample_x_list = np.random.uniform([b[0] for b in bounds_branin], [b[1] for b in bounds_branin], (5, 2))\n",
    "sample_y_list = np.array([branin(x) for x in sample_x_list])\n",
    "\n",
    "#print(sample_x_list)\n",
    "#print(sample_y_list)\n",
    "\n",
    "# Initialize and fit GP model on sampled data\n",
    "gp = GaussianProcess(kernel=rbf_kernel)\n",
    "gp.fit(sample_x_list, sample_y_list)\n",
    "\n",
    "plot_2d_gp_and_acquisition(\n",
    "    true_function=branin,\n",
    "    gp_model=gp,\n",
    "    sample_x_list=sample_x_list,\n",
    "    sample_y_list=sample_y_list,\n",
    "    bounds=bounds_branin,\n",
    "    acquisition_function=expected_improvement,\n",
    "    xi=0.001  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14d75bc-8c9b-475c-96bf-c7862f2c2763",
   "metadata": {},
   "source": [
    "#### YOUR TASK 2: Make Bayesian optimization ABLE TO SOLVE THE PROBLEM! (3P) [IMPORTANT]\n",
    "<a id=\"TASK2\"></a>\n",
    "\n",
    "In the below case, you will see that Bayesian optimization does not perform very well with *3 random sampling + 5 optimization steps*.\n",
    "\n",
    "Your goal is to tune other parameters, such as choosing a different kernel, acquisition function, mean function, etc. \n",
    "\n",
    "**After your modification, the mean_best_y should be at least > 4.6.**\n",
    "\n",
    "You might need to directly change the hyperparameter of kernel or acquisition functions, if that's the case, you can copy and paste your modification in the below cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f612b44-8a03-452e-89d0-f0840604cab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL!!\n",
    "# THIS IS AN EXAMPLE OF FAILING CASE! \n",
    "\n",
    "# Initialize variables for logging best y values\n",
    "best_y_values = []\n",
    "\n",
    "# Repeat Bayesian Optimization 10 times\n",
    "for i in range(20):\n",
    "    print(f\"\\nRun {i + 1} of Bayesian Optimization:\")\n",
    "    # Run Bayesian Optimization in 2D with specified bounds\n",
    "    X_observed, y_observed, gp_model = bayesian_optimization(\n",
    "        target_function=branin,\n",
    "        bounds=bounds_branin,\n",
    "        num_steps=5,\n",
    "        num_random_init=3,\n",
    "        kernel=rbf_kernel,\n",
    "        acquisition_func=\"UCB\",\n",
    "        mean_func=lambda x: 0,\n",
    "        xi=0.000001,\n",
    "        kappa=2.0,\n",
    "        plot_progress=False\n",
    "    )\n",
    "    \n",
    "    # Log the best observed y value from this run\n",
    "    best_y = np.max(y_observed) \n",
    "    best_y_values.append(best_y)\n",
    "    print(f\"Best y from Run {i + 1}: {best_y}\")\n",
    "\n",
    "# Output all best y values\n",
    "print(\"\\nBest y values from each run:\")\n",
    "for idx, best_y in enumerate(best_y_values, start=1):\n",
    "    print(f\"Run {idx}: Best y = {best_y}\")\n",
    "\n",
    "# Calculate and print the mean of the best y values\n",
    "mean_best_y = np.mean(best_y_values)\n",
    "print(f\"\\nMean of the best y values across 20 runs: {mean_best_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d83d13-ecf1-4424-9051-043be176badb",
   "metadata": {},
   "source": [
    "#### Provide your answers in the below cells. \n",
    "\n",
    "In the first cell, you can copy the previous kernels or acquisition functions and tune their hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490668b-d635-4a02-b39c-dfa9ec6b483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For example, you can copy periodic kernel below, and set the default period value as 2. \n",
    "## Just an example! You are encouraged to try out everything. You can create a new kernel or acquisition function, but it's not needed.\n",
    "## By tuning the hyperparameters, the result can be quite promising already.\n",
    "\n",
    "# Your specific periodic kernel\n",
    "def my_periodic_kernel(X1, X2, length_scale=1.0, period=2.0, sigma_f=1.0):\n",
    "    \"\"\"\n",
    "    Periodic kernel for capturing repeating patterns.\n",
    "    \"\"\"\n",
    "    dists = np.sqrt(np.sum((X1[:, None] - X2[None, :])**2, axis=2))\n",
    "    return sigma_f**2 * np.exp(-2 * (np.sin(np.pi * dists / period)**2) / length_scale**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16a7226-63b1-467f-a985-a6cef05076a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell, you can change the parameters that HAVE COMMENTS after. \n",
    "best_y_values = []\n",
    "\n",
    "for i in range(20):\n",
    "    print(f\"\\nRun {i + 1} of Bayesian Optimization:\")\n",
    "    # Run Bayesian Optimization in 2D with specified bounds\n",
    "    X_observed, y_observed, gp_model = bayesian_optimization(\n",
    "        target_function=branin,\n",
    "        bounds=bounds_branin,\n",
    "        num_steps=5,\n",
    "        num_random_init=3,\n",
    "        kernel=my_periodic_kernel,    # YOU CAN CHANGE THIS  (for example, now i use the modified kernel in the above cell)\n",
    "        acquisition_func=\"EI\",        # YOU CAN CHANGE THIS\n",
    "        mean_func=lambda x: 0,        # YOU CAN CHANGE THIS\n",
    "        xi=0.000001,                  # YOU CAN CHANGE THIS\n",
    "        kappa=2.0,                    # YOU CAN CHANGE THIS\n",
    "        plot_progress=False           # Feel free to turn on the plot function to help you complete the task\n",
    "    )\n",
    "    \n",
    "    # Log the best observed y value from this run\n",
    "    best_y = np.max(y_observed) \n",
    "    best_y_values.append(best_y)\n",
    "    print(f\"Best y from Run {i + 1}: {best_y}\")\n",
    "\n",
    "# Output all best y values\n",
    "print(\"\\nBest y values from each run:\")\n",
    "for idx, best_y in enumerate(best_y_values, start=1):\n",
    "    print(f\"Run {idx}: Best y = {best_y}\")\n",
    "\n",
    "# Calculate and print the mean of the best y values\n",
    "mean_best_y = np.mean(best_y_values)\n",
    "print(f\"\\nMean of the best y values across 20 runs: {mean_best_y}\")\n",
    "\n",
    "if(mean_best_y > 4.6):\n",
    "    print(\"Your optimizer has surpassed the set goal.\")\n",
    "else:\n",
    "    print(\"Your optimizer HAS NOT surpassed the set goal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d104d0e-7f37-4ab8-a857-03227460d02c",
   "metadata": {},
   "source": [
    "#### Justification of your code\n",
    "\n",
    "Tell me (a) what did you do, and (b) why you made such changes. \n",
    "\n",
    "Just spend a few sentences, no need to over-explain your decision.\n",
    "\n",
    "#### Your answer starts from here (directly edit beloe in the cell):\n",
    "\n",
    "For example, I hypothesize that the function is periodic, so I chose periodic kernel... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d963d35-02f5-481d-aee1-a22924993f55",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Multi-objective Bayesian optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b7ffa9-b279-4ab8-b815-a944febee00f",
   "metadata": {},
   "source": [
    "### 4.1 Create a second function and visualization\n",
    "\n",
    "We simply flip Branin to create another function. \n",
    "\n",
    "You can compare this function to Branin, which will show you that there is not a single (x1, x2) pair that will satisfy (more technically, maximize) both Branin and reversed Branin functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee850ed-4e4e-4aaf-89af-bdb6d9025b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reversed_branin(x):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - x : array-like, [x1, x2], where x1 and x2 are the normalized input coordinates in [0, 1]^2.\n",
    "    \n",
    "    Returns:\n",
    "    - value : float, the function value at the specified coordinates in the typical Branin range.\n",
    "    \"\"\"\n",
    "    \n",
    "    x1 = x[0]\n",
    "    x2 = 15 - x[1]\n",
    "        \n",
    "    # Branin function calculation\n",
    "    a = 1.0\n",
    "    b = 5.1 / (4 * np.pi**2)\n",
    "    c = 5 / np.pi\n",
    "    r = 6\n",
    "    s = 10\n",
    "    t = 1 / (8 * np.pi)\n",
    "    \n",
    "    return 5 - (a * (x2 - b * x1**2 + c * x1 - r)**2 + s * (1 - t) * np.cos(x1) + s) / 10\n",
    "plot_true_function(reversed_branin, bounds_branin, title=\"True Function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b65182-af6d-4f55-84ae-03e3c7489403",
   "metadata": {},
   "source": [
    "Below are just functions for visualization. No need to edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e4d3fb-4e69-4ad4-9f90-3236c375e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_observations_with_gradient(Y_observed, title=\"Observed 2D Objective Values\", xlabel=\"Objective 1\", ylabel=\"Objective 2\"):\n",
    "    \"\"\"\n",
    "    Plot the observed points in the 2D objective space with a gradient indicating observation sequence.\n",
    "    \n",
    "    Parameters:\n",
    "    - Y_observed : array-like, shape (n_samples, 2)\n",
    "        Observed objective values.\n",
    "    - title : str, optional\n",
    "        Title of the plot.\n",
    "    - xlabel : str, optional\n",
    "        Label for the x-axis.\n",
    "    - ylabel : str, optional\n",
    "        Label for the y-axis.\n",
    "    \"\"\"\n",
    "    # Generate a sequence array to represent the observation order\n",
    "    num_points = Y_observed.shape[0]\n",
    "    sequence = np.linspace(0, 1, num_points)  # Normalize to [0, 1] for color mapping\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(Y_observed[:, 0], Y_observed[:, 1], c=sequence, cmap=\"viridis\", label=\"Observed Points\", edgecolor=\"k\", s=100)\n",
    "    plt.colorbar(scatter, label=\"Observation Sequence\")  # Add color bar to show sequence\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bb1ec8-be0a-488e-be7e-36d4aaf6bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp_prediction(gp_model, bounds, title=\"GP Prediction\", grid_resolution=60):\n",
    "    \"\"\"\n",
    "    Plots the GP model's mean and variance predictions for a single objective.\n",
    "    \n",
    "    Parameters:\n",
    "    - gp_model: GaussianProcess, the trained GP model for the objective.\n",
    "    - bounds: list of tuples, bounds for each input dimension.\n",
    "    - title: str, title for the plot.\n",
    "    - grid_resolution: int, the resolution of the grid for plotting.\n",
    "    \"\"\"\n",
    "    x = np.linspace(bounds[0][0], bounds[0][1], grid_resolution)\n",
    "    y = np.linspace(bounds[1][0], bounds[1][1], grid_resolution)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    xy_grid = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "\n",
    "    # GP Prediction\n",
    "    mean, std = gp_model.predict(xy_grid)\n",
    "    Z_mean = mean.reshape(X.shape)\n",
    "    Z_std = std.reshape(X.shape)\n",
    "\n",
    "    # Plot mean and variance (standard deviation)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Mean prediction\n",
    "    c1 = axs[0].contourf(X, Y, Z_mean, levels=50, cmap=\"viridis\")\n",
    "    fig.colorbar(c1, ax=axs[0], label=\"GP Mean Prediction\")\n",
    "    axs[0].set_title(f\"{title} - Mean\")\n",
    "    axs[0].set_xlabel(\"X1\")\n",
    "    axs[0].set_ylabel(\"X2\")\n",
    "    \n",
    "    # Variance prediction\n",
    "    c2 = axs[1].contourf(X, Y, Z_std, levels=50, cmap=\"plasma\")\n",
    "    fig.colorbar(c2, ax=axs[1], label=\"GP Variance (Std Dev)\")\n",
    "    axs[1].set_title(f\"{title} - Variance\")\n",
    "    axs[1].set_xlabel(\"X1\")\n",
    "    axs[1].set_ylabel(\"X2\")\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_ehvi_acquisition(gp_models, bounds, X_observed, Y_observed, ref_point, grid_resolution=60, title=\"EHVI Acquisition Function\"):\n",
    "    \"\"\"\n",
    "    Plots the EHVI acquisition function across the input space.\n",
    "    \n",
    "    Parameters:\n",
    "    - gp_models: list of GaussianProcess models for each objective.\n",
    "    - bounds: list of tuples, bounds for each input dimension.\n",
    "    - Y_observed: array-like, observed objective values for EHVI.\n",
    "    - ref_point: array-like, reference point for hypervolume calculation.\n",
    "    - grid_resolution: int, resolution of the grid for plotting.\n",
    "    - title: str, title for the plot.\n",
    "    \"\"\"\n",
    "    x = np.linspace(bounds[0][0], bounds[0][1], grid_resolution)\n",
    "    y = np.linspace(bounds[1][0], bounds[1][1], grid_resolution)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    xy_grid = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "\n",
    "    # EHVI Calculation\n",
    "    ehvi_values = expected_hypervolume_improvement(xy_grid, gp_models, X_observed, Y_observed, ref_point)\n",
    "    Z_acq = ehvi_values.reshape(X.shape)\n",
    "\n",
    "    # Plot EHVI\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.contourf(X, Y, Z_acq, levels=50, cmap=\"inferno\")\n",
    "    plt.colorbar(label=\"EHVI Value\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"X1\")\n",
    "    plt.ylabel(\"X2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3152a922-52ad-47b7-bf18-6c1e23cd530d",
   "metadata": {},
   "source": [
    "### 4.2 Basic Components for Multi-Objective Bayesian Optimization (MOBO)\n",
    "\n",
    "MOBO seeks to increase the hypervolume under the Pareto front, capturing the best trade-offs among multiple objectives. Its core components include:\n",
    "\n",
    "1. **Objective Functions and Pareto Front**: \n",
    "   In MOBO, multiple (often conflicting) objective functions are optimized simultaneously. Instead of a single solution, the goal is to find a Pareto front, where no objective can be improved without worsening another. This front represents the set of best trade-offs.\n",
    "\n",
    "2. **Hypervolume Indicator**:\n",
    "   The hypervolume is a measure of the objective space volume dominated by the Pareto front, anchored by a reference point. Maximizing the hypervolume expands this dominated region, improving the Pareto front's quality and diversity.\n",
    "\n",
    "3. **Gaussian Process Models**:\n",
    "   MOBO uses Gaussian Processes (GPs) as surrogate models to approximate each objective function. GPs provide predictions with uncertainty (mean and variance), which guides exploration and refinement of the Pareto front.\n",
    "\n",
    "4. **Acquisition Function (e.g., Expected Hypervolume Improvement)**:\n",
    "   Acquisition functions, such as Expected Hypervolume Improvement (EHVI), balance exploration (unexplored areas) and exploitation (improving known good areas), directing the search towards promising regions to refine the Pareto front.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c24313-975d-4cc4-96f2-b3fdefb3564f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Pareto frontier  [IMPORTANT]\n",
    "\n",
    "In multi-objective optimization, the Pareto front is the set of solutions that are considered optimal when balancing multiple objectives. A solution is part of the Pareto front if it is non-dominated, meaning that no other solution improves all objectives simultaneously. For instance, in the image above, the black points on the boundary represent the Pareto front, as each of these points is optimal in the sense that moving to a different point would involve a trade-off, where improving one objective would worsen the other.\n",
    "\n",
    "#### Hypervolume  [IMPORTANT]\n",
    "\n",
    "The concept of hypervolume builds on this by quantifying the area (or volume in higher dimensions) in objective space that is dominated by the points on the Pareto front, up to a predefined reference point ($v_{ref}$). The reference point represents the \"worst\" values for each objective, providing a baseline from which to measure the space covered by the Pareto front.\n",
    "\n",
    "![Pareto Frontier and Hypervolume](./img/hvs.png)\n",
    "\n",
    "\n",
    "(This image is directly copied from **Pareto Frontier Learning with Expensive Correlated Objectives** by *Shah and Ghahramani*. See http://proceedings.mlr.press/v48/shahc16.pdf)\n",
    "\n",
    "In panel (a) of the image, the light blue region shows the hypervolume dominated by the current Pareto front, covering all points in objective space that are at least as good as one of the Pareto front points (in both objectives) and up to the reference point. This area quantifies the overall \"quality\" of the Pareto front in terms of objective space coverage.\n",
    "\n",
    "In panel (b), we illustrate hypervolume improvement. When a new candidate point $(y_1^{new}, y_2^{new})$ is added to the Pareto front, it expands the dominated region, as shown by the dark blue area. This additional region represents the potential increase in hypervolume and is used to evaluate new points in optimization. By maximizing this Expected Hypervolume Improvement (EHVI), optimization algorithms can prioritize points that will most effectively expand the objective-space coverage, promoting efficient exploration and progress toward optimal solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35e5cda-7bba-496b-8321-54ddcd88d49d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Pareto front and Hypervolume (YOUR TASK 3) [IMPORTANT]\n",
    "<a id=\"TASK3\"></a>\n",
    "\n",
    "Now, we create functions to get pareto front from observed 2D Y values, and the code for computing hypervolume under pareto front.\n",
    "\n",
    "1. **get_pareto_front**\n",
    "\n",
    "   - **Purpose**: This function identifies the set of points (or solutions) on the Pareto front from a given set of observed objective values. In multi-objective optimization, the Pareto front represents the \"best\" points where no single objective can be improved without worsening another.\n",
    "   \n",
    "   - **How it Works**: \n",
    "     - First, it assumes that every point could potentially be on the Pareto front.\n",
    "     - Then, it iterates through each point, checking if any other point \"dominates\" it, meaning the other point performs better in all objectives. \n",
    "     - If a point is dominated by another, it is removed from the Pareto front candidate set.\n",
    "     - Finally, the function returns only the points that are non-dominated and thus lie on the Pareto front.\n",
    "\n",
    "2. **compute_hypervolume (YOUR TASK 3, 3P)**\n",
    "\n",
    "   - **Purpose**: This function calculates the hypervolume dominated by the Pareto front points. Hypervolume is a key measure in multi-objective optimization to assess the quality of the Pareto front—higher hypervolume indicates better coverage of the objective space.\n",
    "\n",
    "   - **How it Works (READ THIS CAREFULLY)**: \n",
    "     - It first sorts the Pareto front points based on the first objective to organize the calculation.\n",
    "     - Then, it calculates the \"width\" (along the x-axis) and \"height\" (along the y-axis) of each rectangle between a Pareto front point and the reference point.\n",
    "     - These rectangles represent the region of objective space dominated by the Pareto front relative to the reference point.\n",
    "     - The total area (sum of all rectangles) represents the hypervolume, which is returned at the end.\n",
    "\n",
    "Please follow the above explanation to complete the hypervolume function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec06c8-cb57-47c6-a42f-7a9408ed3952",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get_pareto_front is correctly implemented. No need to do anything\n",
    "def get_pareto_front(Y):\n",
    "    \"\"\"\n",
    "    Identify the Pareto front from a set of observed objective values in a maximization context.\n",
    "\n",
    "    Parameters:\n",
    "    - Y : array-like, shape (n_samples, n_objectives)\n",
    "        Observed objective values.\n",
    "\n",
    "    Returns:\n",
    "    - pareto_front : array-like, shape (n_pareto, n_objectives)\n",
    "        Points on the Pareto front.\n",
    "    \"\"\"\n",
    "    Y = np.atleast_2d(Y)\n",
    "    pareto_mask = np.ones(Y.shape[0], dtype=bool)  # Start by assuming all points are on the Pareto front\n",
    "\n",
    "    for i, y in enumerate(Y):\n",
    "        if pareto_mask[i]:  # Only check points still on the Pareto front\n",
    "            # If another point strictly dominates `y`, mark `y` as non-Pareto\n",
    "            pareto_mask[i] = not np.any(np.all(Y >= y, axis=1) & np.any(Y > y, axis=1))\n",
    "    return Y[pareto_mask]\n",
    "\n",
    "\n",
    "## YOUR TASK: COMPLETE THE CODE PLEASE!\n",
    "def compute_hypervolume(pareto_front, ref_point):\n",
    "    \"\"\"\n",
    "    Compute the hypervolume of the dominated region for a set of Pareto front points\n",
    "    in a maximization problem with a given reference point.\n",
    "\n",
    "    Parameters:\n",
    "    - pareto_front : array-like, shape (n_pareto, 2)\n",
    "        Points on the Pareto front in two-dimensional objective space.\n",
    "    - ref_point : array-like, shape (2,)\n",
    "        Reference point for hypervolume calculation (typically the lowest possible values).\n",
    "\n",
    "    Returns:\n",
    "    - hypervolume : float\n",
    "        The computed hypervolume.\n",
    "    \"\"\"\n",
    "    # Sort Pareto front points by the first objective in descending order\n",
    "    pareto_front = pareto_front[pareto_front[:, 0].argsort()[::-1]]\n",
    "\n",
    "    hypervolume = 0.0\n",
    "    current_y = ref_point[1]\n",
    "\n",
    "    for point in pareto_front:\n",
    "        hypervolume = 0.0 # Comment out this line after you finish the lines below\n",
    "        # COMPLETE THIS FUNCTION --- IT ONLY NEEDS 4 LINES OF CODE!\n",
    "        # Compute width; i.e., Distance along the x-axis to the reference point\n",
    "        # Compute height; i.e., Distance along the y-axis to the highest y value we've seen so far\n",
    "        # Now, we compute this rectangle, which is basically width * height\n",
    "        # Finally, we move the y boundary up to this Pareto point. Basically, the next rectangle will stack on top of this one. \n",
    "\n",
    "    return hypervolume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d38fd1-b65a-4f0c-8c7d-6c72b9e21311",
   "metadata": {},
   "source": [
    "We can check the ground truth hypervolume of two functions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f31b7-5f38-4dbe-87da-e92ddf0f3e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid search parameters\n",
    "grid_points_per_dimension = 50  # Adjust for finer resolution\n",
    "x1_values = np.linspace(-5, 10, grid_points_per_dimension)\n",
    "x2_values = np.linspace(0, 15, grid_points_per_dimension)\n",
    "X_grid = np.array(np.meshgrid(x1_values, x2_values)).T.reshape(-1, 2)\n",
    "\n",
    "# Evaluate Branin and Reversed Branin on the grid\n",
    "Y_branin = np.array([branin(x) for x in X_grid])\n",
    "Y_reversed_branin = np.array([reversed_branin(x) for x in X_grid])\n",
    "Y_grid = np.column_stack((Y_branin, Y_reversed_branin))\n",
    "\n",
    "# Identify Pareto front on the grid\n",
    "pareto_front = get_pareto_front(Y_grid)\n",
    "\n",
    "# Define the reference point for hypervolume calculation\n",
    "ref_point = np.array([-30, -30])\n",
    "\n",
    "# Compute the hypervolume of the Pareto front found through grid search\n",
    "ground_truth_hypervolume = compute_hypervolume(pareto_front, ref_point)\n",
    "\n",
    "# Display the result\n",
    "# Your implementation should lead to around 1217.14 \n",
    "print(\"Ground truth hypervolume:\", ground_truth_hypervolume)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d073d15a-ce56-4e22-8ec7-19bd3b203036",
   "metadata": {},
   "source": [
    "### 4.3 Acquisition function for learning the Pareto Frontier: Expected Hypervolume Improvement (EHVI)  \n",
    "[VERY IMPORTANT]\n",
    " \n",
    "In multi-objective Bayesian optimization, EHVI evaluates a candidate point (X) based on their expected contribution to increasing the hypervolume of the Pareto front. This is expressed as:\n",
    "\n",
    "$$\n",
    "\\text{EHVI}(X) = \\mathbb{E}[\\text{HV}(P \\cup \\{f(X)\\} - \\text{HV}(P)]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $P$ is the current Pareto front,\n",
    "- $P \\cup \\{f(X)\\}$ represents adding a new point $f(X)$ to this Pareto front in which $X$ is the design setting and $f(X)$ is the resulting $y$ value,\n",
    "- $\\text{HV}$ is the hypervolume measure.\n",
    "\n",
    "To estimate this contribution, we integrate over **exclusive regions** in the Pareto front. Each exclusive region is the part of objective space \"dominated\" by a specific point $y_i$ on the Pareto front but not by any other Pareto front points. \n",
    "\n",
    "The formula becomes:\n",
    "\n",
    "$$\n",
    "\\text{EHVI}(X) = \\sum_{y \\in \\text{Pareto front}} \\int_{\\Omega(y)} \\prod_{i=1}^n \\left[(\\mu_i(X) - y_i) \\Phi(Z_i) + \\sigma_i(x) \\phi(Z_i)\\right] dx\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\Omega(y)$ is the exclusive region of $y$,\n",
    "- $\\mu_i(X)$ and $\\sigma_i(X)$ are the mean and standard deviation predictions of the GP,\n",
    "- $Z_i = \\frac{\\mu_i(X) - y_i}{\\sigma_i(X)}$, with $\\Phi$ and $\\phi$ as the CDF and PDF of a standard normal distribution.\n",
    "- $dx$ is for the integration is over the objective space.\n",
    "\n",
    "The EHVI captures each candidate's expected improvement in terms of hypervolume over the current Pareto front.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f920bc0e-f4fe-4d0b-abf3-8436e914c3da",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### Detailed steps\n",
    "\n",
    "The Expected Hypervolume Improvement (EHVI) quantifies the potential increase in hypervolume when adding a new candidate point, $X$, to the current Pareto front. EHVI is particularly useful in multi-objective Bayesian Optimization as it encourages exploration of solutions that maximize the objective-space volume.\n",
    "\n",
    "1. **Define Improvement Relative to Pareto Front:**\n",
    "   For each candidate point $x$, we predict the Gaussian Process (GP) mean $\\mu$ and standard deviation $\\sigma$ for each objective. For maximization, the improvement relative to each point on the Pareto front, $\\text{pf\\_point}$, is:\n",
    "\n",
    "   $$\n",
    "   \\text{improvement} = \\mu_X - \\text{pf\\_point}\n",
    "   $$\n",
    "\n",
    "   Here:\n",
    "   - $\\mu_x$ is the predicted mean at $x$ for each objective.\n",
    "   - $\\text{pf\\_point}$ represents a point on the current Pareto front for each objective.\n",
    "\n",
    "2. **Standardize Improvement (Z-Score):**\n",
    "   The standardized improvement (Z-score) measures how far the predicted mean $\\mu_x$ exceeds the Pareto front point, relative to the uncertainty $\\sigma_X$:\n",
    "\n",
    "   $$\n",
    "   Z = \\frac{\\text{improvement}}{\\sigma_X + \\epsilon}\n",
    "   $$\n",
    "\n",
    "   where $\\epsilon$ is a small constant added to avoid division by zero.\n",
    "\n",
    "3. **Compute Probability and Expected Improvement:**\n",
    "   Using $Z$, we compute the probability that $X$ will improve upon the Pareto front point $\\text{pf\\_point}$ for each objective. For each objective dimension:\n",
    "   \n",
    "   - **Cumulative Distribution Function (CDF):** Gives the probability that the new point $x$ will improve upon the Pareto front point.\n",
    "   - **Probability Density Function (PDF):** Measures the likelihood of achieving improvement exactly at $Z$.\n",
    "\n",
    "   The product of the CDF and PDF terms captures the expected hypervolume improvement (EHI) for each objective:\n",
    "\n",
    "   $$\n",
    "   \\text{EHI}_{\\text{objective}} = \\text{improvement} \\cdot \\text{CDF}(Z) + \\sigma_X \\cdot \\text{PDF}(Z)\n",
    "   $$\n",
    "\n",
    "4. **Sum EHVI Across All Pareto Points:**\n",
    "   For each candidate $x$, compute the **Expected Hypervolume Improvement (EHVI)** by aggregating the contributions from all regions of the objective space defined by the Pareto front points. The EHVI for each region (corresponding to a Pareto point) considers the improvement in all objectives and is expressed as:\n",
    "\n",
    "   $$\n",
    "   \\text{EHVI}_{\\text{region}}(X, \\text{pf\\_point}) = \\prod_{\\text{objectives}} \\left[\\text{improvement} \\cdot \\text{CDF}(Z) + \\sigma_X \\cdot \\text{PDF}(Z)\\right].\n",
    "   $$\n",
    "\n",
    "   Summing over all Pareto front points gives the total EHVI:\n",
    "\n",
    "   $$\n",
    "   \\text{EHVI}_{\\text{total}}(X) = \\sum_{\\text{pf\\_point} \\in \\text{Pareto front}} \\text{EHVI}_{\\text{region}}(X, \\text{pf\\_point}).\n",
    "   $$\n",
    "\n",
    "5. **Summarize EHVI for $x$:**\n",
    "   Finally, the EHVI for $x$ is obtained by summing the contributions from all regions of the objective space defined by the Pareto front points. This total EHVI quantifies the expected hypervolume increase if $x$ is added.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca59a27d-f8cf-4a06-b681-bbc9f9fc4e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_hypervolume_improvement(X, gp_models, X_observed, Y_observed, ref_point):\n",
    "    \"\"\"\n",
    "    Compute Expected Hypervolume Improvement (EHVI) for each point in X.\n",
    "\n",
    "    Parameters are the same as in the original function.\n",
    "    \"\"\"\n",
    "    X = np.atleast_2d(X)\n",
    "    n_objectives = len(gp_models)\n",
    "    \n",
    "    # Fit each GP model to observed data\n",
    "    for i, gp in enumerate(gp_models):\n",
    "        gp.fit(X_observed, Y_observed[:, i])\n",
    "\n",
    "    pareto_front = get_pareto_front(Y_observed)\n",
    "    mu_list, sigma_list = [], []\n",
    "\n",
    "    for gp in gp_models:\n",
    "        mu, sigma = gp.predict(X)\n",
    "        mu_list.append(mu)\n",
    "        sigma_list.append(sigma)\n",
    "\n",
    "    # Initialize EHVI values\n",
    "    ehvi_values = np.zeros(X.shape[0])\n",
    "\n",
    "    for i, x in enumerate(X):\n",
    "        mu_x = np.array([mu[i] for mu in mu_list])\n",
    "        sigma_x = np.array([sigma[i] for sigma in sigma_list])\n",
    "        hvi_sum = 0\n",
    "\n",
    "        # EHVI calculated over exclusive regions for each Pareto point\n",
    "        for pf_point in pareto_front:\n",
    "            improvement = np.maximum(mu_x - pf_point, 0)\n",
    "            Z = improvement / (sigma_x + 1e-9)\n",
    "            cdf_val = norm.cdf(Z)\n",
    "            pdf_val = norm.pdf(Z)\n",
    "\n",
    "            hvi_sum += np.prod(improvement * cdf_val + sigma_x * pdf_val)\n",
    "\n",
    "        ehvi_values[i] = hvi_sum\n",
    "\n",
    "    return ehvi_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e622c8e-4cf2-46a6-8b7e-90a2cf25b998",
   "metadata": {},
   "source": [
    "### 4.4 Putting it together: MOBO\n",
    "\n",
    "With all the components ready, we can implement our main MOBO function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3179837f-25b6-48a2-8da2-51d0dc51a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Multi-Objective Bayesian Optimization function\n",
    "def multi_objective_bayesian_optimization(\n",
    "        target_functions, bounds, num_steps, num_random_init=5, kernel=rbf_kernel, \n",
    "        acquisition_func=\"EHVI\", ref_point=[-30, -30], plot_progress=True):\n",
    "    \"\"\"\n",
    "    Multi-Objective Bayesian Optimization using EHVI, tracking hypervolume at each step.\n",
    "\n",
    "    Parameters:\n",
    "    - target_functions : list of callables, each being an objective function to maximize.\n",
    "    - bounds : list of tuples, (lower, upper) bounds for each dimension.\n",
    "    - num_steps : int, number of BO iterations.\n",
    "    - num_random_init : int, number of random initial samples.\n",
    "    - kernel : callable, the kernel function for GP models.\n",
    "    - acquisition_func : str, acquisition function (\"EHVI\" supported here).\n",
    "    - ref_point : list, reference point for hypervolume calculation.\n",
    "\n",
    "    Returns:\n",
    "    - X_observed : array of observed x values.\n",
    "    - Y_observed : array of observed objective values.\n",
    "    - pareto_front : array of points on the final Pareto front.\n",
    "    - hypervolumes : list of hypervolume values at each BO iteration.\n",
    "    \"\"\"\n",
    "    num_dims = len(bounds)\n",
    "    num_objectives = len(target_functions)\n",
    "    \n",
    "    # Step 1: Random Initialization\n",
    "    X_observed = np.random.uniform(\n",
    "        [b[0] for b in bounds], [b[1] for b in bounds], size=(num_random_init, num_dims)\n",
    "    )\n",
    "    Y_observed = np.array([[f(x) for f in target_functions] for x in X_observed])\n",
    "\n",
    "    # Initialize the hypervolume tracking list\n",
    "    hypervolumes = []\n",
    "\n",
    "    # Calculate initial hypervolume\n",
    "    initial_pareto_front = get_pareto_front(Y_observed)\n",
    "    initial_hypervolume = compute_hypervolume(initial_pareto_front, ref_point)\n",
    "    hypervolumes.append(initial_hypervolume)\n",
    "\n",
    "    # Step 2: Initialize GP models for each objective\n",
    "    gp_models = [GaussianProcess(kernel=kernel) for _ in range(num_objectives)]\n",
    "    for i, gp in enumerate(gp_models):\n",
    "        gp.fit(X_observed, Y_observed[:, i])\n",
    "\n",
    "    # Step 3: Begin optimization loop\n",
    "    for step in range(num_steps):\n",
    "        # Generate a grid for acquisition function evaluation\n",
    "        grids = [np.linspace(b[0], b[1], 60) for b in bounds]\n",
    "        X_grid = np.array(np.meshgrid(*grids)).T.reshape(-1, num_dims)\n",
    "        \n",
    "        # Step 4: Acquisition Function - EHVI\n",
    "        if acquisition_func == \"EHVI\":\n",
    "            af_values = expected_hypervolume_improvement(X_grid, gp_models, X_observed, Y_observed, ref_point)\n",
    "        \n",
    "        # Step 5: Select the next candidate point\n",
    "        next_x = X_grid[np.argmax(af_values)].reshape(1, -1)\n",
    "        next_y = np.array([f(next_x.flatten()) for f in target_functions])\n",
    "        \n",
    "        # Step 6: Update observations\n",
    "        X_observed = np.vstack((X_observed, next_x))\n",
    "        Y_observed = np.vstack((Y_observed, next_y))\n",
    "        \n",
    "        # Update GP models with new data\n",
    "        for i, gp in enumerate(gp_models):\n",
    "            gp.fit(X_observed, Y_observed[:, i])\n",
    "\n",
    "        # Update and track hypervolume\n",
    "        pareto_front = get_pareto_front(Y_observed)\n",
    "        hypervolume = compute_hypervolume(pareto_front, ref_point)\n",
    "        hypervolumes.append(hypervolume)\n",
    "\n",
    "        if plot_progress:\n",
    "            # Plot the GP prediction for each objective\n",
    "            plot_gp_prediction(gp_models[0], bounds=bounds, title=\"GP Prediction for Objective 1\")\n",
    "            plot_gp_prediction(gp_models[1], bounds=bounds, title=\"GP Prediction for Objective 2\")\n",
    "\n",
    "            # Plot the EHVI acquisition function\n",
    "            plot_ehvi_acquisition(gp_models, bounds=bounds, X_observed=X_observed, Y_observed=Y_observed, ref_point=ref_point)\n",
    "\n",
    "        print(f\"Step {step + 1}: x = {next_x.flatten()}, y = {next_y}, hypervolume = {hypervolume}\")\n",
    "\n",
    "    # Step 7: Return final results\n",
    "    pareto_front = get_pareto_front(Y_observed)\n",
    "    return X_observed, Y_observed, pareto_front, hypervolumes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4ac00a-4989-4442-b462-06b30de78b6d",
   "metadata": {},
   "source": [
    "Deploy our MOBO for optimizing both Branin and reversed_Branin in the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e40bc73-4b4a-48cd-8bdd-e2d0b3d54ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up target functions and reference point\n",
    "target_functions = [branin, reversed_branin]\n",
    "ref_point = [-30, -30]  # Reference point for hypervolume calculation\n",
    "\n",
    "init_steps = 15\n",
    "opt_steps = 15\n",
    "\n",
    "MOBO_round = 10\n",
    "\n",
    "all_hvs = []\n",
    "all_y_observed = []\n",
    "for i in range(MOBO_round):\n",
    "    print(f\"\\nRun {i + 1} of MOBO:\")\n",
    "    # Run Multi-Objective Bayesian Optimization\n",
    "    X_observed, Y_observed, pareto_front, hvs = multi_objective_bayesian_optimization(\n",
    "        target_functions=target_functions,\n",
    "        bounds=bounds_branin,\n",
    "        num_steps=opt_steps,  \n",
    "        num_random_init=init_steps,\n",
    "        kernel=matern_kernel,\n",
    "        acquisition_func=\"EHVI\",\n",
    "        ref_point=ref_point,\n",
    "        plot_progress=False\n",
    "    )\n",
    "    all_hvs.append(hvs)\n",
    "    all_y_observed.append(Y_observed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9252e52c-1e0f-48aa-a8f4-516ec28dd543",
   "metadata": {},
   "source": [
    "#### Visualize the search behavior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242a8b95-0587-459e-9360-f068ea41850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (MOBO_round):\n",
    "    plot_2d_observations_with_gradient(all_y_observed[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac9d720-68ad-4dbb-ac22-91245cc08c8a",
   "metadata": {},
   "source": [
    "### 4.5 A comparison baseline: only random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2e7e1c-f109-44cb-9d24-24fe1026aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_objective_random_search(\n",
    "        target_functions, bounds, num_searches, ref_point=[-30, -30]):\n",
    "    \"\"\"\n",
    "    Multi-Objective Random Search, tracking hypervolume at each step.\n",
    "\n",
    "    Parameters:\n",
    "    - target_functions : list of callables, each being an objective function to maximize.\n",
    "    - bounds : list of tuples, (lower, upper) bounds for each dimension.\n",
    "    - num_searches : int, number of random search iterations.\n",
    "    - ref_point : list, reference point for hypervolume calculation.\n",
    "\n",
    "    Returns:\n",
    "    - X_observed : array of observed x values.\n",
    "    - Y_observed : array of observed objective values.\n",
    "    - pareto_front : array of points on the final Pareto front.\n",
    "    - hypervolumes : list of hypervolume values at each iteration.\n",
    "    \"\"\"\n",
    "    num_dims = len(bounds)\n",
    "    num_objectives = len(target_functions)\n",
    "\n",
    "    # Random Initialization\n",
    "    X_observed = np.random.uniform(\n",
    "        [b[0] for b in bounds], [b[1] for b in bounds], size=(num_searches, num_dims)\n",
    "    )\n",
    "    Y_observed = np.array([[f(x) for f in target_functions] for x in X_observed])\n",
    "\n",
    "    # Initialize the hypervolume tracking list\n",
    "    hypervolumes = []\n",
    "\n",
    "    # Track hypervolume at each iteration\n",
    "    for i in range(num_searches):\n",
    "        # Get the current Pareto front\n",
    "        pareto_front = get_pareto_front(Y_observed[:i+1])\n",
    "        # Compute the hypervolume\n",
    "        hypervolume = compute_hypervolume(pareto_front, ref_point)\n",
    "        hypervolumes.append(hypervolume)\n",
    "        \n",
    "        print(f\"Random Search {i + 1}: x = {X_observed[i]}, y = {Y_observed[i]}, hypervolume = {hypervolume}\")\n",
    "\n",
    "    # Final Pareto front after all searches\n",
    "    final_pareto_front = get_pareto_front(Y_observed)\n",
    "    \n",
    "    return X_observed, Y_observed, final_pareto_front, hypervolumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a06ec-ce5e-425d-84e9-45d67cda095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up target functions and reference point\n",
    "target_functions = [branin, reversed_branin]\n",
    "ref_point = [-30, -30]  # Reference point for hypervolume calculation\n",
    "\n",
    "random_hvs = []\n",
    "random_y_observed = []\n",
    "for i in range(MOBO_round):\n",
    "    print(f\"\\nRun {i + 1} of random search:\")\n",
    "    # Run Multi-Objective Bayesian Optimization\n",
    "    X_observed, Y_observed, pareto_front, hvs = multi_objective_random_search(\n",
    "        target_functions=target_functions,\n",
    "        bounds=bounds_branin,\n",
    "        num_searches=init_steps + opt_steps\n",
    "    )\n",
    "    random_hvs.append(hvs)\n",
    "    random_y_observed.append(Y_observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d0b2a-4be3-4d14-97e5-763b13c9d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(MOBO_round):\n",
    "    plot_2d_observations_with_gradient(random_y_observed[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413c3669-4fd8-4d39-9d64-548ed1a43ced",
   "metadata": {},
   "source": [
    "#### Plotting the hypervolume progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382a7ed2-c003-4d4b-9838-64829a0bd52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists of hypervolumes to numpy arrays for easier calculations\n",
    "random_hvs_array = np.array(random_hvs)  # Shape: (5 runs, 15 iterations)\n",
    "mobo_hvs_array = np.array(all_hvs)       # Shape: (5 runs, 10 iterations)\n",
    "\n",
    "# Compute mean and std for each iteration count\n",
    "random_hv_means = random_hvs_array[:, init_steps - 1:].mean(axis=0)  # Starting from iteration 5\n",
    "random_hv_stds = random_hvs_array[:, init_steps - 1:].std(axis=0)\n",
    "\n",
    "mobo_hv_means = mobo_hvs_array.mean(axis=0)\n",
    "mobo_hv_stds = mobo_hvs_array.std(axis=0)\n",
    "\n",
    "# Define iteration counts for x-axis\n",
    "iterations_random = np.arange(init_steps , init_steps + opt_steps + 1)  # For random search\n",
    "iterations_mobo = np.arange(init_steps , init_steps + opt_steps + 1)    # For MOBO (to align with random search's length)\n",
    "\n",
    "# Plot the mean hypervolumes with error bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(iterations_random, random_hv_means, yerr=random_hv_stds, fmt='-o', capsize=3, label=\"Random Search\")\n",
    "plt.errorbar(iterations_mobo, mobo_hv_means, yerr=mobo_hv_stds, fmt='-o', capsize=3, color=\"orange\", label=\"MOBO\")\n",
    "\n",
    "# Labeling the plot\n",
    "plt.xlabel(\"Iteration Count\")\n",
    "plt.ylabel(\"Mean Hypervolume\")\n",
    "plt.title(\"Mean Hypervolume Progression with 1 Std Dev Error Bars\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876682d5-15a5-4cf8-8646-4bf32f6a8c31",
   "metadata": {},
   "source": [
    "## 5. Reflection (YOUR TASK 4, 1P)\n",
    "\n",
    "<a id=\"TASK4\"></a>\n",
    "\n",
    "For your own project, try to come up with one thing that Bayesian optimization can be useful for helping you make a decision. Your answer will need to include the detailed description of the problem, the design parameters, and the objective functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234bbb4a-ee4a-4d06-9b2b-52308809d5c6",
   "metadata": {},
   "source": [
    "#### Fill in your answer below (in the cell):\n",
    "\n",
    "**Problem (what do you want to optimize for):** \n",
    "\n",
    "**Design parameters (what parameters may have an influence):**\n",
    "\n",
    "**Objective functions (what metrics/measurements you want to improve):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a8a0ba-8cf5-4a6e-a952-b9fd7111dd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
